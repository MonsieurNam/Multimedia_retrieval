import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
import re
import torch
from tqdm import tqdm
from typing import Dict, List, Optional, Any

# Import BasicSearcher Ä‘á»ƒ sá»­ dá»¥ng lÃ m ná»n táº£ng
from search_core.basic_searcher import BasicSearcher

# Import thÆ° viá»‡n Gemini vÃ  decorator retrier
import google.generativeai as genai
from utils import gemini_api_retrier

class SemanticSearcher:
    """
    Class thá»±c hiá»‡n tÃ¬m kiáº¿m ngá»¯ nghÄ©a nÃ¢ng cao.

    Chá»‹u trÃ¡ch nhiá»‡m chÃ­nh cho viá»‡c tÃ¬m kiáº¿m dá»±a trÃªn ná»™i dung (KIS) vÃ  tÃ¬m kiáº¿m bá»‘i cáº£nh cho cÃ¡c tÃ¡c vá»¥ khÃ¡c.
    Bao gá»“m cÃ¡c bÆ°á»›c:
    1.  Truy xuáº¥t á»©ng viÃªn ban Ä‘áº§u báº±ng CLIP Ä‘a ngÃ´n ngá»¯.
    2.  TÄƒng cÆ°á»ng truy váº¥n báº±ng Gemini Ä‘á»ƒ trÃ­ch xuáº¥t thá»±c thá»ƒ vÃ  bá»‘i cáº£nh.
    3.  TÃ¡i xáº¿p háº¡ng (rerank) cÃ¡c á»©ng viÃªn dá»±a trÃªn cÃ´ng thá»©c Ä‘iá»ƒm káº¿t há»£p 3 yáº¿u tá»‘:
        CLIP (hÃ¬nh áº£nh), Object (Ä‘á»‘i tÆ°á»£ng), vÃ  Semantic (ngá»¯ cáº£nh/transcript).
    """

    def __init__(self, 
                 basic_searcher: BasicSearcher, 
                 device: str = "cuda"):
        """
        Khá»Ÿi táº¡o SemanticSearcher.

        Args:
            basic_searcher (BasicSearcher): Má»™t instance cá»§a BasicSearcher Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o.
            device (str): Thiáº¿t bá»‹ Ä‘á»ƒ cháº¡y model ('cuda' hoáº·c 'cpu').
        """
        print("--- ðŸ§  Khá»Ÿi táº¡o SemanticSearcher (PhiÃªn báº£n NÃ¢ng cao) ---")
        self.device = device
        self.basic_searcher = basic_searcher
        # gemini_model sáº½ Ä‘Æ°á»£c gÃ¡n tá»« bÃªn ngoÃ i bá»Ÿi MasterSearcher
        self.gemini_model: Optional[genai.GenerativeModel] = None

        print("   -> Äang táº£i mÃ´ hÃ¬nh Bi-Encoder tiáº¿ng Viá»‡t ('bkai-foundation-models/vietnamese-bi-encoder')...")
        self.semantic_model = SentenceTransformer(
            'bkai-foundation-models/vietnamese-bi-encoder', 
            device=self.device
        )
        print("--- âœ… Táº£i model Bi-Encoder thÃ nh cÃ´ng! ---")
        
    @gemini_api_retrier(max_retries=3, initial_delay=2)
    def _gemini_enhance_call(self, prompt: str):
        """HÃ m con Ä‘Æ°á»£c "trang trÃ­", chá»‰ Ä‘á»ƒ thá»±c hiá»‡n lá»‡nh gá»i API Gemini."""
        safety_settings = {
            "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
            "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE",
            "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
            "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE",
        }
        response = self.gemini_model.generate_content(prompt, safety_settings=safety_settings)
        return response

    def enhance_query_with_gemini(self, query: str) -> Dict[str, Any]:
        """
        Sá»­ dá»¥ng Gemini Ä‘á»ƒ phÃ¢n tÃ­ch má»™t truy váº¥n, cÃ³ thá»ƒ lÃ  KIS hoáº·c VQA.

        Returns:
            Dict[str, Any]: Má»™t dictionary chá»©a 'search_context', 'specific_question', 
                            'objects_vi', 'objects_en'.
        """
        fallback_result = {
            'search_context': query,
            'specific_question': "" if "?" not in query else query,
            'objects_vi': query.split(),
            'objects_en': query.split()
        }
        
        if not self.gemini_model:
            print("--- âš ï¸ Gemini model chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o. Sá»­ dá»¥ng fallback cho enhance_query. ---")
            return fallback_result

        prompt = f"""
        You are an expert query analyzer for a Vietnamese video search system. Your task is to analyze a user query and break it down into structured components. The query can be a simple scene description (KIS) or a question about a scene (VQA).

        Return ONLY a single, valid JSON object with four keys: "search_context", "specific_question", "objects_vi", and "objects_en".

        **Rules:**
        - "search_context": A descriptive phrase in Vietnamese for finding the relevant scene. For VQA, this is the scene the question is about. For KIS, it's the query itself.
        - "specific_question": The specific question being asked. For KIS queries, this should be an empty string "".
        - "objects_vi": A list of important Vietnamese nouns/entities from the "search_context".
        - "objects_en": The direct English translation for EACH item in "objects_vi". The two lists must have the same length.

        **Example 1 (VQA):**
        Query: "Trong video quay cáº£nh bá»¯a tiá»‡c, ngÆ°á»i phá»¥ ná»¯ máº·c vÃ¡y Ä‘á» Ä‘ang cáº§m ly mÃ u gÃ¬?"
        JSON: {{"search_context": "cáº£nh bá»¯a tiá»‡c cÃ³ ngÆ°á»i phá»¥ ná»¯ máº·c vÃ¡y Ä‘á»", "specific_question": "cÃ´ áº¥y Ä‘ang cáº§m ly mÃ u gÃ¬?", "objects_vi": ["bá»¯a tiá»‡c", "ngÆ°á»i phá»¥ ná»¯", "vÃ¡y Ä‘á»"], "objects_en": ["party", "woman", "red dress"]}}

        **Example 2 (KIS):**
        Query: "má»™t chiáº¿c xe cá»©u há»a Ä‘ang chá»¯a chÃ¡y tÃ²a nhÃ "
        JSON: {{"search_context": "má»™t chiáº¿c xe cá»©u há»a Ä‘ang chá»¯a chÃ¡y tÃ²a nhÃ ", "specific_question": "", "objects_vi": ["xe cá»©u há»a", "tÃ²a nhÃ "], "objects_en": ["fire truck", "building"]}}

        **Query to process:** "{query}"
        **JSON:**
        """
        try:
            response = self._gemini_enhance_call(prompt)
            match = re.search(r"\{.*\}", response.text, re.DOTALL)
            if not match:
                raise ValueError("No JSON object found in Gemini response.")
            
            result = json.loads(match.group(0))
            
            if all(k in result for k in ['search_context', 'specific_question', 'objects_vi', 'objects_en']) and \
               len(result['objects_vi']) == len(result['objects_en']):
                print(f"--- âœ… PhÃ¢n tÃ­ch truy váº¥n thÃ nh cÃ´ng. Context: '{result['search_context']}' ---")
                return result

            print("--- âš ï¸ JSON tá»« Gemini khÃ´ng há»£p lá»‡. Sá»­ dá»¥ng fallback. ---")
            return fallback_result
        except Exception as e:
            print(f"--- âš ï¸ Lá»—i khi gá»i API Gemini Ä‘á»ƒ tÄƒng cÆ°á»ng truy váº¥n: {e}. Sá»­ dá»¥ng fallback. ---")
            return fallback_result

    def search(self, 
               query_text: str, 
               top_k_final: int = 12, 
               top_k_retrieval: int = 100, 
               precomputed_analysis: Optional[Dict] = None
              ) -> List[Dict[str, Any]]:
        """
        Thá»±c hiá»‡n pipeline tÃ¬m kiáº¿m ngá»¯ nghÄ©a hoÃ n chá»‰nh cho má»™t context.

        Args:
            query_text (str): Bá»‘i cáº£nh tÃ¬m kiáº¿m (search_context).
            top_k_final (int): Sá»‘ káº¿t quáº£ cuá»‘i cÃ¹ng tráº£ vá».
            top_k_retrieval (int): Sá»‘ á»©ng viÃªn ban Ä‘áº§u Ä‘Æ°á»£c láº¥y tá»« FAISS.
            precomputed_analysis (Optional[Dict]): Káº¿t quáº£ phÃ¢n tÃ­ch Gemini Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c.

        Returns:
            List[Dict[str, Any]]: Danh sÃ¡ch cÃ¡c keyframe káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c xáº¿p háº¡ng.
        """
        print(f"\n--- Báº¯t Ä‘áº§u Semantic Search cho context: '{query_text}' ---")
        
        # --- BÆ°á»›c 1: Truy xuáº¥t á»©ng viÃªn báº±ng CLIP ---
        candidates = self.basic_searcher.search(query_text, top_k=top_k_retrieval)
        if not candidates:
            return []

        # --- BÆ°á»›c 2: Láº¥y thÃ´ng tin PhÃ¢n tÃ­ch & TÄƒng cÆ°á»ng ---
        # Æ¯u tiÃªn sá»­ dá»¥ng káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c tá»« MasterSearcher
        if precomputed_analysis:
            enhanced_query = precomputed_analysis
        else:
            enhanced_query = self.enhance_query_with_gemini(query_text)
        
        rerank_keywords_en = enhanced_query.get('objects_en', [])
        # LuÃ´n sá»­ dá»¥ng 'search_context' Ä‘á»ƒ rerank, vÃ¬ Ä‘Ã³ lÃ  pháº§n mÃ´ táº£ cáº£nh
        rerank_context_vi = enhanced_query.get('search_context', query_text).lower().strip()
        
        # --- BÆ°á»›c 3: TÃ¡i xáº¿p háº¡ng (Reranking) ---
        context_vector = self.semantic_model.encode(rerank_context_vi, convert_to_tensor=True, device=self.device)
        candidate_texts = [cand.get('searchable_text', '') for cand in candidates]
        candidate_vectors = self.semantic_model.encode(
            candidate_texts, convert_to_tensor=True, device=self.device, 
            batch_size=128, show_progress_bar=False # Táº¯t progress bar Ä‘á»ƒ log Ä‘á»¡ rá»‘i
        )
        semantic_scores_tensor = util.pytorch_cos_sim(context_vector, candidate_vectors)[0]
        
        query_object_vectors = None
        if rerank_keywords_en:
            query_object_vectors = self.semantic_model.encode(
                rerank_keywords_en, convert_to_tensor=True, device=self.device)

        reranked_results = []
        for i, cand in enumerate(candidates):
            object_score = 0.0
            detected_objects_en_raw = cand.get('objects_detected', [])
            detected_objects_en = list(detected_objects_en_raw) if isinstance(detected_objects_en_raw, np.ndarray) else list(detected_objects_en_raw)

            if query_object_vectors is not None and len(detected_objects_en) > 0:
                detected_object_vectors = self.semantic_model.encode(
                    detected_objects_en, convert_to_tensor=True, device=self.device)
                cosine_scores = util.pytorch_cos_sim(query_object_vectors, detected_object_vectors)
                if cosine_scores.numel() > 0:
                    max_scores_per_query_obj = torch.max(cosine_scores, dim=1).values
                    object_score = torch.mean(max_scores_per_query_obj).item()

            semantic_score = (semantic_scores_tensor[i].item() + 1) / 2
            
            w_clip, w_obj, w_semantic = 0.4, 0.3, 0.3
            normalized_clip_score = cand['clip_score']
            
            final_score = (w_clip * normalized_clip_score + 
                           w_obj * object_score + 
                           w_semantic * semantic_score)
            
            cand['final_score'] = final_score
            cand['scores'] = {'clip': normalized_clip_score, 'object': object_score, 'semantic': semantic_score}
            reranked_results.append(cand)

        reranked_results = sorted(reranked_results, key=lambda x: x['final_score'], reverse=True)
        print(f"--- âœ… TÃ¡i xáº¿p háº¡ng cho context '{query_text}' hoÃ n táº¥t! ---")
        
        return reranked_results[:top_k_final]