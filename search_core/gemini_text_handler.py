import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from typing import Dict, Any, List, Set
import json
import re

from utils import api_retrier

class GeminiTextHandler:
    """
    M·ªôt class chuy√™n d·ª•ng ƒë·ªÉ x·ª≠ l√Ω T·∫§T C·∫¢ c√°c t√°c v·ª• li√™n quan ƒë·∫øn vƒÉn b·∫£n
    b·∫±ng API c·ªßa Google Gemini (c·ª• th·ªÉ l√† model Flash).
    
    Bao g·ªìm: ph√¢n lo·∫°i t√°c v·ª•, ph√¢n t√≠ch chi ti·∫øt truy v·∫•n, v√† ph√¢n r√£ truy v·∫•n TRAKE.
    """

    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        """
        Kh·ªüi t·∫°o Gemini Text Handler.

        Args:
            api_key (str): Google API Key.
            model_name (str): T√™n model Gemini s·∫Ω s·ª≠ d·ª•ng.
        """
        print(f"--- ‚ú® Kh·ªüi t·∫°o Gemini Text Handler v·ªõi model: {model_name} ---")
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name)
            self.known_entities_prompt_segment = ""
            self.health_check() # Th·ª±c hi·ªán health check ngay khi kh·ªüi t·∫°o
            print("--- ‚úÖ Gemini Text Handler ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o v√† x√°c th·ª±c th√†nh c√¥ng! ---")
        except Exception as e:
            print(f"--- ‚ùå L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o Gemini Text Handler: {e} ---")
            raise e

    @api_retrier(max_retries=3, initial_delay=1)
    def _gemini_text_call(self, prompt: str):
        """H√†m con ƒë∆∞·ª£c "trang tr√≠", ch·ªâ ƒë·ªÉ th·ª±c hi·ªán l·ªánh g·ªçi API text c·ªßa Gemini."""
        safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        generation_config = genai.types.GenerationConfig(response_mime_type="application/json")
        response = self.model.generate_content(prompt, safety_settings=safety_settings, generation_config=generation_config)
        return response

    def health_check(self):
        """Th·ª±c hi·ªán m·ªôt l·ªánh g·ªçi API ƒë∆°n gi·∫£n ƒë·ªÉ ki·ªÉm tra key v√† k·∫øt n·ªëi."""
        print("--- ü©∫ ƒêang th·ª±c hi·ªán ki·ªÉm tra tr·∫°ng th√°i API Gemini... ---")
        try:
            self.model.count_tokens("ki·ªÉm tra")
            print("--- ‚úÖ Tr·∫°ng th√°i API Gemini: OK ---")
            return True
        except Exception as e:
            print(f"--- ‚ùå L·ªói API Gemini: {e} ---")
            raise e

    def analyze_task_type(self, query: str) -> str:
        """Ph√¢n lo·∫°i truy v·∫•n b·∫±ng Gemini, s·ª≠ d·ª•ng prompt c√≥ Quy t·∫Øc ∆Øu ti√™n."""
        prompt = f"""
        You are a highly precise query classifier. Your task is to classify a Vietnamese query into one of four categories: TRACK_VQA, TRAKE, QNA, or KIS. You MUST follow a strict priority order.

        **Priority Order for Classification (Check from top to bottom):**

        1.  **Check for TRACK_VQA first:** Does the query ask a question about a COLLECTION of items, requiring aggregation (counting, listing, summarizing)? Look for keywords like "ƒë·∫øm", "bao nhi√™u", "li·ªát k√™", "t·∫•t c·∫£", "m·ªói", or plural subjects. If it matches, classify as **TRACK_VQA** and stop.
            - Example: "trong bu·ªïi tr√¨nh di·ªÖn m√∫a l√¢n, ƒë·∫øm xem c√≥ bao nhi√™u con l√¢n" -> This is a request to count a collection, so it is **TRACK_VQA**.

        2.  **Then, check for TRAKE:** If it's not TRACK_VQA, does the query ask for a SEQUENCE of DIFFERENT, ordered actions? Look for patterns like "(1)...(2)...", "b∆∞·ªõc 1... b∆∞·ªõc 2", "sau ƒë√≥". If it matches, classify as **TRAKE** and stop.
            - Example: "ng∆∞·ªùi ƒë√†n √¥ng ƒë·ª©ng l√™n r·ªìi b∆∞·ªõc ƒëi"

        3.  **Then, check for QNA:** If not TRACK_VQA or TRAKE, does the query ask a **direct question** that expects a factual answer about something in the scene? This is more than just describing a scene. Look for:
            - **Interrogative words:** "ai", "c√°i g√¨", "·ªü ƒë√¢u", "khi n√†o", "t·∫°i sao", "nh∆∞ th·∫ø n√†o", "m√†u g√¨", "h√£ng n√†o", etc.
            - **Question structures:** "c√≥ ph·∫£i l√†...", "ƒëang l√†m g√¨", "l√† ai", "tr√¥ng nh∆∞ th·∫ø n√†o".
            - A question mark "?".
            If it matches, classify as **QNA** and stop.
            - Example: "ng∆∞·ªùi ph·ª• n·ªØ m·∫∑c √°o m√†u g√¨?" -> QNA
            - Example: "ai l√† ng∆∞·ªùi ƒë√†n √¥ng ƒëang ph√°t bi·ªÉu?" -> QNA
            - Example: "c√≥ bao nhi√™u chi·∫øc xe tr√™n ƒë∆∞·ªùng?" -> This asks for a count of a single scene, so it is **QNA**. (Distinguish this from TRACK_VQA which counts across multiple scenes/videos).

        4.  **Default to KIS:** If the query is a statement or a descriptive phrase looking for a moment, classify as **KIS**. It describes "what to find", not "what to answer".
            - Example: "c·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒëang ph√°t bi·ªÉu" -> KIS
            - Example: "t√¨m ng∆∞·ªùi ph·ª• n·ªØ m·∫∑c √°o ƒë·ªè" -> KIS
            - Example: "m·ªôt chi·∫øc xe ƒëang ch·∫°y" -> KIS

        **Your Task:**
        Follow the priority order strictly. Analyze the query below and return ONLY the final category as a single word.

        **Query:** "{query}"
        **Category:**
        """
        try:
            response = self._gemini_text_call(prompt)
            task_type = response.text.strip().upper()
            if task_type in ["KIS", "QNA", "TRAKE", "TRACK_VQA"]:
                return task_type
            return "KIS"
        except Exception:
            return "KIS" # Fallback an to√†n

    def analyze_query_fully(self, query: str) -> Dict[str, Any]:
        """
        Th·ª±c hi·ªán ph√¢n t√≠ch to√†n di·ªán m·ªôt truy v·∫•n, bao g·ªìm c·∫£ ph√¢n lo·∫°i t√°c v·ª•,
        trong M·ªòT l·∫ßn g·ªçi API duy nh·∫•t, y√™u c·∫ßu output d·∫°ng JSON.
        """
        fallback_result = {
            'task_type': 'KIS', 'search_context': query, 'specific_question': "",
            'aggregation_instruction': "", 'objects_vi': [], 'objects_en': []
        }
        
        prompt = f"""
        You are a master Vietnamese query analyzer for a video search system. Analyze the user's query and return ONLY a single, valid JSON object with ALL the following keys: "task_type", "search_context", "specific_question", "aggregation_instruction", "objects_vi", "objects_en".

        **Analysis Steps & Rules:**

        1.  **Determine `task_type` FIRST (Strict Priority):**
            - **TRACK_VQA:** Does it ask to aggregate (count, list, summarize) info about a COLLECTION of items across scenes? (e.g., "ƒë·∫øm s·ªë l√¢n", "li·ªát k√™ t·∫•t c·∫£ c√°c xe"). If yes, `task_type` is "TRACK_VQA".
            - **TRAKE:** If not, does it ask for a SEQUENCE of distinct actions? (e.g., "ƒë·ª©ng l√™n r·ªìi ƒëi ra"). If yes, `task_type` is "TRAKE".
            - **QNA:** If not, is it a DIRECT QUESTION expecting a factual answer about a single scene? (e.g., "ai l√† ng∆∞·ªùi...", "m√†u g√¨?", "ƒëang l√†m g√¨?"). If yes, `task_type` is "QNA".
            - **KIS:** Otherwise, it's a descriptive search. `task_type` is "KIS".

        2.  **Fill other keys based on `task_type`:**
            - `search_context`: The general scene to search for. ALWAYS FILL THIS.
            - `specific_question`: The specific question for a vision model. ONLY for QNA and TRACK_VQA.
            - `aggregation_instruction`: The final goal. ONLY for TRACK_VQA.
            - `objects_vi` & `objects_en`: Key nouns/entities from the query.

        **Example 1 (QNA):**
        Query: "ai l√† ng∆∞·ªùi ƒë√†n √¥ng ƒë·ªôi m≈© ƒë·ªè ƒëang ph√°t bi·ªÉu ·ªü m·ªπ"
        JSON: {{"task_type": "QNA", "search_context": "c·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë·ªôi m≈© ƒë·ªè ƒëang ph√°t bi·ªÉu ·ªü M·ªπ", "specific_question": "ai l√† ng∆∞·ªùi ƒë√†n √¥ng n√†y?", "aggregation_instruction": "", "objects_vi": ["ng∆∞·ªùi ƒë√†n √¥ng", "m≈© ƒë·ªè", "ph√°t bi·ªÉu", "M·ªπ"], "objects_en": ["man", "red hat", "speaking", "USA"]}}

        **Example 2 (KIS):**
        Query: "c·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë·ªôi m≈© ƒë·ªè ph√°t bi·ªÉu ·ªü m·ªπ"
        JSON: {{"task_type": "KIS", "search_context": "c·∫£nh ng∆∞·ªùi ƒë√†n √¥ng ƒë·ªôi m≈© ƒë·ªè ph√°t bi·ªÉu ·ªü M·ªπ", "specific_question": "", "aggregation_instruction": "", "objects_vi": ["ng∆∞·ªùi ƒë√†n √¥ng", "m≈© ƒë·ªè", "ph√°t bi·ªÉu", "M·ªπ"], "objects_en": ["man", "red hat", "speaking", "USA"]}}
        ---
        **Your Task:**
        Analyze the query below and generate the required JSON object.

        **Query:** "{query}"
        **JSON:**
        """
        try:
            response = self._gemini_text_call(prompt)
            raw_text = response.text
            match = re.search(r"```json\s*(\{.*?\})\s*```", raw_text, re.DOTALL)
            json_string = match.group(1) if match else raw_text
            
            result = json.loads(json_string)
            if 'task_type' in result and 'search_context' in result:
                return result
            # N·∫øu JSON h·ª£p l·ªá nh∆∞ng thi·∫øu key, tr·∫£ v·ªÅ fallback nh∆∞ng v·∫´n gi·ªØ l·∫°i nh·ªØng g√¨ c√≥
            return {**fallback_result, **result}

        except Exception as e:
            print(f"L·ªói Gemini analyze_query_fully: {e}. Response: '{getattr(response, 'text', 'N/A')}'")
            return fallback_result

    def enhance_query(self, query: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t th√¥ng tin truy v·∫•n b·∫±ng Gemini."""
        fallback_result = {
            'search_context': query, 'specific_question': "", 'aggregation_instruction': "",
            'objects_vi': [], 'objects_en': []
        }
        prompt = f"""
        Analyze a Vietnamese user query for a video search system. **Return ONLY a valid JSON object** with five keys: "search_context", "specific_question", "aggregation_instruction", "objects_vi", and "objects_en".

        **Rules:**
        1.  `search_context`: A Vietnamese phrase for finding the general scene. This is used for vector search.
        2.  `specific_question`: The specific question to ask the Vision model for EACH individual frame.
        3.  `aggregation_instruction`: The final instruction for the AI to synthesize all individual answers. This should reflect the user's ultimate goal (e.g., counting, listing, summarizing).
        4.  `objects_vi`: A list of Vietnamese nouns/entities.
        5.  `objects_en`: The English translation for EACH item in `objects_vi`.

        **Example (VQA):**
        Query: "Trong video quay c·∫£nh b·ªØa ti·ªác, ng∆∞·ªùi ph·ª• n·ªØ m·∫∑c v√°y ƒë·ªè ƒëang c·∫ßm ly m√†u g√¨?"
        JSON: {{"search_context": "c·∫£nh b·ªØa ti·ªác c√≥ ng∆∞·ªùi ph·ª• n·ªØ m·∫∑c v√°y ƒë·ªè", "specific_question": "c√¥ ·∫•y ƒëang c·∫ßm ly m√†u g√¨?", "aggregation_instruction": "tr·∫£ l·ªùi c√¢u h·ªèi ng∆∞·ªùi ph·ª• n·ªØ c·∫ßm ly m√†u g√¨", "objects_vi": ["b·ªØa ti·ªác", "ng∆∞·ªùi ph·ª• n·ªØ", "v√°y ƒë·ªè"], "objects_en": ["party", "woman", "red dress"]}}

        **Example (Track-VQA):**
        Query: "ƒë·∫øm xem c√≥ bao nhi√™u con l√¢n trong bu·ªïi bi·ªÉu di·ªÖn"
        JSON: {{"search_context": "bu·ªïi bi·ªÉu di·ªÖn m√∫a l√¢n", "specific_question": "c√≥ con l√¢n n√†o trong ·∫£nh n√†y kh√¥ng v√† m√†u g√¨?", "aggregation_instruction": "t·ª´ c√°c quan s√°t, ƒë·∫øm t·ªïng s·ªë l√¢n v√† li·ªát k√™ m√†u s·∫Øc c·ªßa ch√∫ng", "objects_vi": ["con l√¢n", "bu·ªïi bi·ªÉu di·ªÖn"], "objects_en": ["lion dance", "performance"]}}

        **Your Task:**
        Analyze the query below and generate the JSON.

        **Query:** "{query}"
        **JSON:**
        """
        try:
            response = self._gemini_text_call(prompt)
            # Tr√≠ch xu·∫•t JSON t·ª´ markdown block (Gemini th∆∞·ªùng tr·∫£ v·ªÅ nh∆∞ v·∫≠y)
            match = re.search(r"```json\s*(\{.*?\})\s*```", response.text, re.DOTALL)
            if not match:
                match = re.search(r"(\{.*?\})", response.text, re.DOTALL) # Th·ª≠ t√¨m JSON kh√¥ng c√≥ markdown
            
            if match:
                result = json.loads(match.group(1))
                # Validate ...
                return result
            return fallback_result
        except Exception as e:
            print(f"L·ªói Gemini enhance_query: {e}")
            return fallback_result
            
    def decompose_trake_query(self, query: str) -> List[str]:
        """Ph√¢n r√£ truy v·∫•n TRAKE b·∫±ng Gemini."""
        prompt = f"""
        Decompose the Vietnamese query describing a sequence of actions into a JSON array of short, self-contained phrases. Return ONLY the JSON array.

        Example:
        Query: "T√¨m 4 kho·∫£nh kh·∫Øc ch√≠nh khi v·∫≠n ƒë·ªông vi√™n th·ª±c hi·ªán c√∫ nh·∫£y: (1) gi·∫≠m nh·∫£y, (2) bay qua x√†, (3) ti·∫øp ƒë·∫•t, (4) ƒë·ª©ng d·∫≠y."
        JSON: ["v·∫≠n ƒë·ªông vi√™n gi·∫≠m nh·∫£y", "v·∫≠n ƒë·ªông vi√™n bay qua x√†", "v·∫≠n ƒë·ªông vi√™n ti·∫øp ƒë·∫•t", "v·∫≠n ƒë·ªông vi√™n ƒë·ª©ng d·∫≠y"]

        Query: "{query}"
        JSON:
        """
        try:
            response = self._gemini_text_call(prompt)
            match = re.search(r"\[.*?\]", response.text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
            return [query]
        except Exception:
            return [query]
        
    def load_known_entities(self, known_entities: Set[str]):
        """
        Chu·∫©n b·ªã v√† cache l·∫°i ph·∫ßn prompt ch·ª©a t·ª´ ƒëi·ªÉn ƒë·ªëi t∆∞·ª£ng.
        Ch·ªâ c·∫ßn g·ªçi m·ªôt l·∫ßn khi MasterSearcher kh·ªüi t·∫°o.
        """
        if not known_entities:
            print("--- ‚ö†Ô∏è T·ª´ ƒëi·ªÉn ƒë·ªëi t∆∞·ª£ng r·ªóng. Semantic Grounding s·∫Ω kh√¥ng ho·∫°t ƒë·ªông. ---")
            return
        
        # S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o prompt nh·∫•t qu√°n gi·ªØa c√°c l·∫ßn ch·∫°y
        sorted_entities = sorted(list(known_entities))
        # ƒê·ªãnh d·∫°ng th√†nh chu·ªói JSON ƒë·ªÉ nh√∫ng v√†o prompt
        self.known_entities_prompt_segment = json.dumps(sorted_entities)
        print(f"--- ‚úÖ GeminiTextHandler: ƒê√£ n·∫°p {len(sorted_entities)} th·ª±c th·ªÉ v√†o b·ªô nh·ªõ prompt. ---")

    def perform_semantic_grounding(self, user_objects: List[str]) -> List[str]:
        """
        S·ª≠ d·ª•ng Gemini ƒë·ªÉ "d·ªãch" c√°c ƒë·ªëi t∆∞·ª£ng c·ªßa ng∆∞·ªùi d√πng sang c√°c th·ª±c th·ªÉ ƒë√£ bi·∫øt.
        """
        if not self.known_entities_prompt_segment or not user_objects:
            return user_objects # Tr·∫£ v·ªÅ nh∆∞ c≈© n·∫øu kh√¥ng c√≥ t·ª´ ƒëi·ªÉn ho·∫∑c kh√¥ng c√≥ object

        prompt = f"""
        You are an AI assistant for a video search engine. Your task is to map a list of user-provided objects to a predefined list of "known entities".

        Rules:
        1. For each object in the user's list, find the BEST, most specific, single corresponding entity from the "Known Entities" list.
        2. If an object is a specific type of a known entity, map it to the general entity (e.g., "poodle" -> "dog", "lamborghini" -> "car").
        3. If an object is already a known entity, keep it as is.
        4. If an object has NO reasonable mapping in the known list (e.g., "love", "happiness"), discard it.
        5. The final list should not have duplicate entities.
        6. Return ONLY a valid JSON array containing the final, mapped entities. Your entire response must be a single JSON array.

        **Known Entities:**
        {self.known_entities_prompt_segment}

        **Example 1:**
        User Objects: ["lamborghini", "a woman in a red shirt", "poodle"]
        Expected JSON Result: ["car", "woman", "shirt", "dog"]

        **Example 2:**
        User Objects: ["happiness", "a big cat"]
        Expected JSON Result: ["cat"]
        ---
        **Your Task:**
        User Objects: {json.dumps(user_objects)}
        Analyze and produce the JSON Result:
        """
        
        try:
            response = self._gemini_text_call(prompt)
            # V√¨ ƒë√£ y√™u c·∫ßu response_mime_type="application/json", ch√∫ng ta c√≥ th·ªÉ parse tr·ª±c ti·∫øp
            # response.text s·∫Ω l√† m·ªôt chu·ªói JSON
            # Th√™m m·ªôt l·ªõp b·∫£o v·ªá ƒë·ªÉ tr√≠ch xu·∫•t JSON t·ª´ markdown block n·∫øu c√≥
            raw_text = response.text
            match = re.search(r"```json\s*(\[.*?\])\s*```", raw_text, re.DOTALL)
            json_string = match.group(1) if match else raw_text

            mapped_objects = json.loads(json_string)
            
            if isinstance(mapped_objects, list):
                # Ki·ªÉm tra th√™m ƒë·ªÉ ƒë·∫£m b·∫£o c√°c ph·∫ßn t·ª≠ l√† string
                if all(isinstance(item, str) for item in mapped_objects):
                    return mapped_objects
            
            print(f"--- ‚ö†Ô∏è Semantic Grounding: K·∫øt qu·∫£ tr·∫£ v·ªÅ kh√¥ng ph·∫£i l√† m·ªôt list of strings. Fallback. Got: {mapped_objects} ---")
            return user_objects

        except (json.JSONDecodeError, ValueError) as e:
             print(f"--- ‚ö†Ô∏è Semantic Grounding: Kh√¥ng th·ªÉ parse JSON t·ª´ Gemini. L·ªói: {e}. Response: '{response.text}' ---")
             return user_objects
        except Exception as e:
            print(f"--- ‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh trong l√∫c th·ª±c hi·ªán Semantic Grounding: {e} ---")
            return user_objects