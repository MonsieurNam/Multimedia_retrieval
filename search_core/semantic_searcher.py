import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
import re
import torch
from tqdm import tqdm
from typing import Dict, List, Optional, Any

# Import BasicSearcher ƒë·ªÉ s·ª≠ d·ª•ng l√†m n·ªÅn t·∫£ng
from .basic_searcher import BasicSearcher

# Import th∆∞ vi·ªán Gemini
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

class SemanticSearcher:
    """
    Class th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ nghƒ©a n√¢ng cao.

    Bao g·ªìm c√°c b∆∞·ªõc:
    1.  Truy xu·∫•t ·ª©ng vi√™n ban ƒë·∫ßu b·∫±ng CLIP ƒëa ng√¥n ng·ªØ (th√¥ng qua BasicSearcher).
    2.  TƒÉng c∆∞·ªùng truy v·∫•n b·∫±ng Gemini ƒë·ªÉ tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† b·ªëi c·∫£nh.
    3.  T√°i x·∫øp h·∫°ng (rerank) c√°c ·ª©ng vi√™n d·ª±a tr√™n m·ªôt c√¥ng th·ª©c ƒëi·ªÉm k·∫øt h·ª£p:
        - ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng CLIP (h√¨nh ·∫£nh).
        - ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a c·ªßa ƒë·ªëi t∆∞·ª£ng (object).
        - ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a c·ªßa b·ªëi c·∫£nh (context/transcript).
    """

    def __init__(self, 
                 basic_searcher: BasicSearcher, 
                 gemini_model: Optional[genai.GenerativeModel] = None, 
                 device: str = "cuda"):
        """
        Kh·ªüi t·∫°o SemanticSearcher.

        Args:
            basic_searcher (BasicSearcher): M·ªôt instance c·ªßa BasicSearcher ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.
            gemini_model (Optional[genai.GenerativeModel]): Instance c·ªßa Gemini model ƒë·ªÉ chia s·∫ª.
            device (str): Thi·∫øt b·ªã ƒë·ªÉ ch·∫°y model ('cuda' ho·∫∑c 'cpu').
        """
        print("--- üß† Kh·ªüi t·∫°o SemanticSearcher (Phi√™n b·∫£n N√¢ng cao) ---")
        self.device = device
        self.basic_searcher = basic_searcher
        self.gemini_model = gemini_model

        print("   -> ƒêang t·∫£i m√¥ h√¨nh Bi-Encoder ti·∫øng Vi·ªát ('bkai-foundation-models/vietnamese-bi-encoder')...")
        self.semantic_model = SentenceTransformer(
            'bkai-foundation-models/vietnamese-bi-encoder', 
            device=self.device
        )
        print("--- ‚úÖ T·∫£i model Bi-Encoder th√†nh c√¥ng! ---")
        
    def enhance_query_with_gemini(self, query: str) -> Dict[str, Any]:
        """
        S·ª≠ d·ª•ng Gemini ƒë·ªÉ ph√¢n t√≠ch, tƒÉng c∆∞·ªùng v√† d·ªãch truy v·∫•n c·ªßa ng∆∞·ªùi d√πng.

        Args:
            query (str): C√¢u truy v·∫•n g·ªëc b·∫±ng ti·∫øng Vi·ªát.

        Returns:
            Dict[str, Any]: M·ªôt dictionary ch·ª©a 'objects_vi', 'objects_en', v√† 'context_vi'.
        """
        fallback_result = {'objects_vi': query.split(), 'objects_en': query.split(), 'context_vi': query}
        
        if not self.gemini_model:
            print("--- ‚ö†Ô∏è Gemini model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. S·ª≠ d·ª•ng fallback cho enhance_query. ---")
            return fallback_result

        prompt = f"""
        You are a helpful assistant for a Vietnamese video search system. Your task is to analyze a Vietnamese user query and extract key information.
        Return ONLY a single, valid JSON object with three keys: "objects_vi", "objects_en", and "context_vi".
        - "objects_vi": A list of important nouns and entities from the query in Vietnamese.
        - "objects_en": The direct English translation for EACH item in "objects_vi". The two lists must have the same length.
        - "context_vi": A simple sentence in Vietnamese that describes the main action or context of the query.

        **Example 1:**
        Query: "c√¥ g√°i m·∫∑c v√°y v√†ng ƒëi d·∫°o trong c√¥ng vi√™n g·∫ßn b·ªù h·ªì"
        JSON: {{"objects_vi": ["c√¥ g√°i", "v√°y v√†ng", "c√¥ng vi√™n", "b·ªù h·ªì"], "objects_en": ["girl", "yellow dress", "park", "lakeshore"], "context_vi": "m·ªôt c√¥ g√°i ƒëang ƒëi d·∫°o trong c√¥ng vi√™n"}}

        **Example 2:**
        Query: "xe c·ª©u h·ªèa phun n∆∞·ªõc ch·ªØa ch√°y t√≤a nh√†"
        JSON: {{"objects_vi": ["xe c·ª©u h·ªèa", "n∆∞·ªõc", "t√≤a nh√†"], "objects_en": ["fire truck", "water", "building"], "context_vi": "m·ªôt chi·∫øc xe c·ª©u h·ªèa ƒëang ch·ªØa ch√°y"}}

        **Query to process:** "{query}"
        **JSON:**
        """
        safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        try:
            response = self.gemini_model.generate_content(prompt, safety_settings=safety_settings)
            match = re.search(r"\{.*\}", response.text, re.DOTALL)
            if not match:
                raise ValueError("No JSON object found in Gemini response.")
            
            result = json.loads(match.group(0))
            
            if all(k in result for k in ['objects_vi', 'objects_en', 'context_vi']) and \
               isinstance(result['objects_vi'], list) and \
               isinstance(result['objects_en'], list) and \
               len(result['objects_vi']) == len(result['objects_en']):
                return result

            print("--- ‚ö†Ô∏è JSON t·ª´ Gemini kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng fallback. ---")
            return fallback_result
        except Exception as e:
            print(f"--- ‚ö†Ô∏è L·ªói khi g·ªçi API Gemini ƒë·ªÉ tƒÉng c∆∞·ªùng truy v·∫•n: {e}. S·ª≠ d·ª•ng fallback. ---")
            return fallback_result

    def search(self, 
               query_text: str, 
               top_k_final: int = 12, 
               top_k_retrieval: int = 100, 
               precomputed_analysis: Optional[Dict] = None
              ) -> List[Dict[str, Any]]:
        """
        Th·ª±c hi·ªán pipeline t√¨m ki·∫øm ng·ªØ nghƒ©a ho√†n ch·ªânh.

        Args:
            query_text (str): C√¢u truy v·∫•n c·ªßa ng∆∞·ªùi d√πng.
            top_k_final (int): S·ªë k·∫øt qu·∫£ cu·ªëi c√πng tr·∫£ v·ªÅ.
            top_k_retrieval (int): S·ªë ·ª©ng vi√™n ban ƒë·∫ßu ƒë∆∞·ª£c l·∫•y t·ª´ FAISS.
            precomputed_analysis (Optional[Dict]): K·∫øt qu·∫£ ph√¢n t√≠ch Gemini ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n tr∆∞·ªõc.

        Returns:
            List[Dict[str, Any]]: Danh s√°ch c√°c keyframe k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c x·∫øp h·∫°ng.
        """
        print(f"\n--- B·∫Øt ƒë·∫ßu Semantic Search cho: '{query_text}' ---")
        
        # --- B∆∞·ªõc 1: Truy xu·∫•t ·ª©ng vi√™n b·∫±ng CLIP ---
        print(f"1. Truy xu·∫•t Top {top_k_retrieval} ·ª©ng vi√™n b·∫±ng CLIP...")
        candidates = self.basic_searcher.search(query_text, top_k=top_k_retrieval)
        if not candidates:
            print("-> Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n n√†o.")
            return []
        print(f"-> T√¨m th·∫•y {len(candidates)} ·ª©ng vi√™n.")

        # --- B∆∞·ªõc 2: Ph√¢n t√≠ch & TƒÉng c∆∞·ªùng truy v·∫•n ---
        if precomputed_analysis:
            print("\n2. S·ª≠ d·ª•ng k·∫øt qu·∫£ ph√¢n t√≠ch truy v·∫•n ƒë√£ c√≥...")
            enhanced_query = precomputed_analysis
        else:
            print("\n2. TƒÉng c∆∞·ªùng v√† D·ªãch truy v·∫•n b·∫±ng Gemini...")
            enhanced_query = self.enhance_query_with_gemini(query_text)
        
        rerank_keywords_en = enhanced_query.get('objects_en', [])
        rerank_context_vi = enhanced_query.get('context_vi', '').lower().strip()
        
        print(f" -> English Keywords for Reranking: {rerank_keywords_en}")
        print(f" -> Vietnamese Context for Reranking: '{rerank_context_vi}'")

        # --- B∆∞·ªõc 3: T√°i x·∫øp h·∫°ng (Reranking) ---
        print("\n3. B·∫Øt ƒë·∫ßu t√°i x·∫øp h·∫°ng c√°c ·ª©ng vi√™n...")
        
        # M√£ h√≥a ng·ªØ c·∫£nh truy v·∫•n m·ªôt l·∫ßn
        context_vector = self.semantic_model.encode(rerank_context_vi, convert_to_tensor=True, device=self.device)
        
        # M√£ h√≥a c√°c vƒÉn b·∫£n c·ªßa ·ª©ng vi√™n theo batch ƒë·ªÉ tƒÉng t·ªëc
        candidate_texts = [cand.get('searchable_text', '') for cand in candidates]
        candidate_vectors = self.semantic_model.encode(
            candidate_texts, 
            convert_to_tensor=True, 
            device=self.device, 
            batch_size=128, 
            show_progress_bar=True
        )
        # T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a cho t·∫•t c·∫£ ·ª©ng vi√™n c√πng l√∫c
        semantic_scores_tensor = util.pytorch_cos_sim(context_vector, candidate_vectors)[0]
        
        # M√£ h√≥a c√°c t·ª´ kh√≥a object trong truy v·∫•n m·ªôt l·∫ßn
        query_object_vectors = None
        if rerank_keywords_en:
            query_object_vectors = self.semantic_model.encode(
                rerank_keywords_en, 
                convert_to_tensor=True, 
                device=self.device
            )

        reranked_results = []
        for i, cand in enumerate(tqdm(candidates, desc="Calculating Final Scores")):
            # --- T√≠nh Object Score ---
            object_score = 0.0
            
            # L·∫•y danh s√°ch object, ƒë·∫£m b·∫£o n√≥ l√† list
            detected_objects_en_raw = cand.get('objects_detected', [])
            
            # Chuy·ªÉn ƒë·ªïi an to√†n t·ª´ NumPy array (n·∫øu c√≥) sang list
            if isinstance(detected_objects_en_raw, np.ndarray):
                detected_objects_en = detected_objects_en_raw.tolist()
            else:
                # N·∫øu n√≥ ƒë√£ l√† list ho·∫∑c ki·ªÉu d·ªØ li·ªáu kh√°c, chuy·ªÉn n√≥ th√†nh list
                detected_objects_en = list(detected_objects_en_raw)

            # --- D√íNG C·∫¶N S·ª¨A ƒê√ÇY ---
            # S·ª≠a t·ª´: `if query_object_vectors is not None and detected_objects_en and len(detected_objects_en) > 0:`
            # Th√†nh: `if query_object_vectors is not None and len(detected_objects_en) > 0:`
            if query_object_vectors is not None and len(detected_objects_en) > 0:
                detected_object_vectors = self.semantic_model.encode(
                    detected_objects_en, 
                    convert_to_tensor=True, 
                    device=self.device
                )
                
                # So s√°nh m·ªói object truy v·∫•n v·ªõi t·∫•t c·∫£ object ƒë∆∞·ª£c ph√°t hi·ªán
                cosine_scores = util.pytorch_cos_sim(query_object_vectors, detected_object_vectors)
                
                # Ki·ªÉm tra ƒë·ªÉ ƒë·∫£m b·∫£o cosine_scores kh√¥ng r·ªóng tr∆∞·ªõc khi th·ª±c hi·ªán max()
                if cosine_scores.numel() > 0:
                    # V·ªõi m·ªói object truy v·∫•n, t√¨m ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng cao nh·∫•t
                    max_scores_per_query_obj = torch.max(cosine_scores, dim=1).values
                    # ƒêi·ªÉm object l√† trung b√¨nh c·ªßa c√°c ƒëi·ªÉm cao nh·∫•t n√†y
                    object_score = torch.mean(max_scores_per_query_obj).item()

            # --- T√≠nh Semantic Score ---
            # Chu·∫©n h√≥a ƒëi·ªÉm cosine similarity (t·ª´ [-1, 1]) v·ªÅ thang [0, 1]
            semantic_score = (semantic_scores_tensor[i].item() + 1) / 2
            
            # --- T√≠nh Final Score ---
            w_clip, w_obj, w_semantic = 0.4, 0.3, 0.3
            # Gi·∫£ s·ª≠ clip_score t·ª´ BasicSearcher ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a trong thang [0,1]
            normalized_clip_score = cand['clip_score']
            
            final_score = (w_clip * normalized_clip_score + 
                           w_obj * object_score + 
                           w_semantic * semantic_score)
            
            cand['final_score'] = final_score
            cand['scores'] = {'clip': normalized_clip_score, 'object': object_score, 'semantic': semantic_score}
            reranked_results.append(cand)

        # S·∫Øp x·∫øp l·∫°i d·ª±a tr√™n ƒëi·ªÉm cu·ªëi c√πng v√† tr·∫£ v·ªÅ
        reranked_results = sorted(reranked_results, key=lambda x: x['final_score'], reverse=True)
        print("--- ‚úÖ T√°i x·∫øp h·∫°ng ho√†n t·∫•t! ---")
        
        return reranked_results[:top_k_final]