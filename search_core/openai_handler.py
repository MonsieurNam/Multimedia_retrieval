# ==============================================================================
# OPENAI HANDLER - PHI√äN B·∫¢N N√ÇNG C·∫§P V2 (D·ª∞A TR√äN B·∫¢N G·ªêC + CONTEXT-AWARE VQA)
# ==============================================================================
import openai
import json
import re
import base64
from typing import Dict, Any, List, Optional
import io
from PIL import Image
from utils import api_retrier

class OpenAIHandler:
    """
    M·ªôt class "adapter" ƒë·ªÉ ƒë√≥ng g√≥i t·∫•t c·∫£ c√°c l·ªánh g·ªçi API ƒë·∫øn OpenAI.
    Che gi·∫•u s·ª± ph·ª©c t·∫°p c·ªßa vi·ªác g·ªçi API v√† cung c·∫•p c√°c ph∆∞∆°ng th·ª©c
    r√µ r√†ng cho c√°c t√°c v·ª• c·ª• th·ªÉ (ph√¢n t√≠ch, VQA, etc.).
    """
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        # --- KH√îNG THAY ƒê·ªîI ---
        print(f"--- ü§ñ Kh·ªüi t·∫°o OpenAI Handler v·ªõi model m·∫∑c ƒë·ªãnh: {model} ---")
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.vision_model = "gpt-4o"
        
    @api_retrier(max_retries=2, initial_delay=1)
    def check_api_health(self) -> bool:
        # --- KH√îNG THAY ƒê·ªîI ---
        print("--- ü©∫ ƒêang th·ª±c hi·ªán ki·ªÉm tra tr·∫°ng th√°i API OpenAI... ---")
        try:
            self.client.embeddings.create(input="ki·ªÉm tra", model="text-embedding-3-small")
            print("--- ‚úÖ Tr·∫°ng th√°i API OpenAI: OK ---")
            return True
        except openai.AuthenticationError as e:
            print(f"--- ‚ùå L·ªói OpenAI API: Authentication Error. API Key c√≥ th·ªÉ kh√¥ng h·ª£p l·ªá. L·ªói: {e} ---")
            return False
        except Exception as e:
            print(f"--- ‚ùå L·ªói OpenAI API: Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn OpenAI. L·ªói: {e} ---")
            return False

    @api_retrier(max_retries=3, initial_delay=2)
    def _openai_vision_call(self, messages: List[Dict], is_json: bool = True, is_vision: bool = False) -> str:
        # --- KH√îNG THAY ƒê·ªîI ---
        model_to_use = self.vision_model if is_vision else self.model
        response_format = {"type": "json_object"} if is_json else {"type": "text"}
        
        response = self.client.chat.completions.create(
            model=model_to_use, messages=messages, response_format=response_format,
            temperature=0.1, max_tokens=1024
        )
        
        if response.choices and response.choices[0].message:
            content = response.choices[0].message.content
            return content if content is not None else "" 
        
        return ""

    def _preprocess_and_encode_image(
        self, 
        image_path: str, 
        max_size: int = 1024, 
        quality: int = 90
    ) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω ·∫£nh (resize, n√©n) v√† m√£ h√≥a sang Base64.
        ƒê√¢y l√† b∆∞·ªõc t·ªëi quan tr·ªçng ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô tin c·∫≠y v√† hi·ªáu su·∫•t.
        """
        try:
            with Image.open(image_path) as img:
                # 1. Chu·∫©n h√≥a: Chuy·ªÉn sang ƒë·ªãnh d·∫°ng RGB ƒë·ªÉ lo·∫°i b·ªè c√°c k√™nh m√†u ph·ª©c t·∫°p nh∆∞ RGBA ho·∫∑c CMYK.
                if img.mode != 'RGB':
                    img = img.convert('RGB')

                # 2. Resize: Gi·∫£m k√≠ch th∆∞·ªõc ·∫£nh xu·ªëng m·ªôt ng∆∞·ª°ng h·ª£p l√Ω trong khi gi·ªØ nguy√™n t·ª∑ l·ªá.
                #    GPT-4o x·ª≠ l√Ω t·ªët nh·∫•t v·ªõi ·∫£nh ~1024px.
                img.thumbnail((max_size, max_size))

                # 3. N√©n & L∆∞u v√†o b·ªô nh·ªõ ƒë·ªám (in-memory buffer)
                #    Kh√¥ng c·∫ßn l∆∞u ra file t·∫°m, gi√∫p tƒÉng t·ªëc ƒë·ªô.
                buffer = io.BytesIO()
                img.save(buffer, format="JPEG", quality=quality)
                img_bytes = buffer.getvalue()

                # 4. M√£ h√≥a Base64
                return base64.b64encode(img_bytes).decode('utf-8')
        except FileNotFoundError:
            print(f"--- ‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω ·∫£nh: File kh√¥ng t·ªìn t·∫°i t·∫°i '{image_path}' ---")
            return ""
        except Exception as e:
            print(f"--- ‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω ·∫£nh {image_path}: {e} ---")
            return ""

    # === H√ÄM ƒê√É ƒê∆Ø·ª¢C N√ÇNG C·∫§P ===
    def perform_vqa(self, image_path: str, question: str, context_text: Optional[str] = None) -> Dict[str, any]:
        """
        Th·ª±c hi·ªán VQA s·ª≠ d·ª•ng GPT-4o, c√≥ th·ªÉ nh·∫≠n th√™m b·ªëi c·∫£nh t·ª´ transcript.
        *** PHI√äN B·∫¢N C√ì X·ª¨ L√ù L·ªñI T·ªêT H∆†N V√Ä B·ªêI C·∫¢NH M·ªû R·ªòNG ***
        """
        base64_image = self._preprocess_and_encode_image(image_path)
        if not base64_image:
            return {"answer": "L·ªói: Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh", "confidence": 0.0}

        # *** B·∫ÆT ƒê·∫¶U N√ÇNG C·∫§P T·∫†I ƒê√ÇY ***
        
        context_prompt_part = ""
        has_context = False
        if context_text and context_text.strip():
            has_context = True
            truncated_context = (context_text[:500] + '...') if len(context_text) > 503 else context_text
            context_prompt_part = f"""
        **Additional Context from Transcript (What was said around this moment):**
        ---
        "{truncated_context}"
        ---
        """
        
        # Th√™m log m·ªõi
        if has_context:
            print(f"   -> üß† Th·ª±c hi·ªán VQA v·ªõi Context: '{question}'")
        
        prompt = f"""
        Analyze the image and use the provided transcript context (if any) to answer the question in Vietnamese.
        Return a JSON object with two keys: "answer" (string) and "confidence" (float).
        {context_prompt_part}
        Question: "{question}"
        """
        
        messages = [
            {"role": "user", "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
            ]}
        ]

        try:
            response_content = self._openai_vision_call(messages, is_json=True, is_vision=True)
            
            if not response_content:
                print("--- ‚ö†Ô∏è OpenAI VQA kh√¥ng tr·∫£ v·ªÅ n·ªôi dung. ---")
                return {"answer": "Kh√¥ng th·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh", "confidence": 0.1}

            result = json.loads(response_content)
            return {
                "answer": result.get("answer", "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi"),
                "confidence": float(result.get("confidence", 0.5))
            }
        except (json.JSONDecodeError, TypeError) as e:
            print(f"L·ªói OpenAI perform_vqa (JSON parsing): {e}. Response nh·∫≠n ƒë∆∞·ª£c: '{response_content}'")
            return {"answer": "L·ªói ƒë·ªãnh d·∫°ng ph·∫£n h·ªìi", "confidence": 0.0}
        except Exception as e:
            print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh trong OpenAI perform_vqa: {e}")
            return {"answer": "L·ªói x·ª≠ l√Ω VQA", "confidence": 0.0}

    def decompose_trake_query(self, query: str) -> List[str]:
        # --- KH√îNG THAY ƒê·ªîI ---
        prompt = f"""
        Decompose the Vietnamese query...
        ...
        """
        try:
            response_content = self._openai_vision_call([{"role": "user", "content": prompt}], is_json=True)
            result = json.loads(response_content)
            if isinstance(result, list):
                return result
            return [query]
        except Exception as e:
            print(f"L·ªói OpenAI decompose_trake_query: {e}")
            return [query]