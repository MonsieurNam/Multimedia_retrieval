# ==============================================================================
# OPENAI HANDLER - PHIÃŠN Báº¢N NÃ‚NG Cáº¤P V2 (Dá»°A TRÃŠN Báº¢N Gá»C + CONTEXT-AWARE VQA)
# ==============================================================================
import openai
import json
import re
import base64
from typing import Dict, Any, List, Optional
import io
from PIL import Image
from utils import api_retrier
import os

class OpenAIHandler:
    """
    Má»™t class "adapter" Ä‘á»ƒ Ä‘Ã³ng gÃ³i táº¥t cáº£ cÃ¡c lá»‡nh gá»i API Ä‘áº¿n OpenAI.
    Che giáº¥u sá»± phá»©c táº¡p cá»§a viá»‡c gá»i API vÃ  cung cáº¥p cÃ¡c phÆ°Æ¡ng thá»©c
    rÃµ rÃ ng cho cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ (phÃ¢n tÃ­ch, VQA, etc.).
    """
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        # --- KHÃ”NG THAY Äá»”I ---
        print(f"--- ğŸ¤– Khá»Ÿi táº¡o OpenAI Handler vá»›i model máº·c Ä‘á»‹nh: {model} ---")
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.vision_model = "gpt-4o"
        
    @api_retrier(max_retries=2, initial_delay=1)
    def check_api_health(self) -> bool:
        # --- KHÃ”NG THAY Äá»”I ---
        print("--- ğŸ©º Äang thá»±c hiá»‡n kiá»ƒm tra tráº¡ng thÃ¡i API OpenAI... ---")
        try:
            self.client.embeddings.create(input="kiá»ƒm tra", model="text-embedding-3-small")
            print("--- âœ… Tráº¡ng thÃ¡i API OpenAI: OK ---")
            return True
        except openai.AuthenticationError as e:
            print(f"--- âŒ Lá»—i OpenAI API: Authentication Error. API Key cÃ³ thá»ƒ khÃ´ng há»£p lá»‡. Lá»—i: {e} ---")
            return False
        except Exception as e:
            print(f"--- âŒ Lá»—i OpenAI API: KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n OpenAI. Lá»—i: {e} ---")
            return False

    @api_retrier(max_retries=3, initial_delay=2)
    def _openai_vision_call(self, messages: List[Dict], is_json: bool = True, is_vision: bool = False) -> str:
        # --- KHÃ”NG THAY Äá»”I ---
        model_to_use = self.vision_model if is_vision else self.model
        response_format = {"type": "json_object"} if is_json else {"type": "text"}
        
        response = self.client.chat.completions.create(
            model=model_to_use, messages=messages, response_format=response_format,
            temperature=0.1, max_tokens=1024
        )
        
        if response.choices and response.choices[0].message:
            content = response.choices[0].message.content
            return content if content is not None else "" 
        
        return ""

    def _preprocess_and_encode_image(
        self, 
        image_path: str, 
        resize_threshold_mb: float = 1.0, # Chá»‰ resize náº¿u áº£nh lá»›n hÆ¡n 1MB
        max_dimension: int = 2048, # Giá»›i háº¡n kÃ­ch thÆ°á»›c tá»‘i Ä‘a má»›i, lá»›n hÆ¡n
        quality: int = 95 # TÄƒng cháº¥t lÆ°á»£ng nÃ©n
    ) -> str:
        """
        Tiá»n xá»­ lÃ½ áº£nh Má»˜T CÃCH THÃ”NG MINH vÃ  mÃ£ hÃ³a sang Base64.
        NÃ³ chá»‰ resize vÃ  nÃ©n láº¡i náº¿u kÃ­ch thÆ°á»›c file gá»‘c vÆ°á»£t quÃ¡ ngÆ°á»¡ng.
        """
        try:
            # --- BÆ°á»›c 1: Kiá»ƒm tra kÃ­ch thÆ°á»›c file gá»‘c ---
            file_size_mb = os.path.getsize(image_path) / (1024 * 1024)

            # --- BÆ°á»›c 2: Xá»­ lÃ½ ---
            # Ká»ŠCH Báº¢N 1: áº¢nh Ä‘á»§ nhá» vÃ  an toÃ n -> Gá»­i Ä‘i nguyÃªn báº£n
            if file_size_mb <= resize_threshold_mb:
                print(f"   -> áº¢nh '{os.path.basename(image_path)}' ({file_size_mb:.2f}MB) Ä‘á»§ nhá», gá»­i nguyÃªn báº£n.")
                with open(image_path, "rb") as image_file:
                    return base64.b64encode(image_file.read()).decode('utf-8')

            # Ká»ŠCH Báº¢N 2: áº¢nh quÃ¡ lá»›n -> Cáº§n tiá»n xá»­ lÃ½
            else:
                print(f"   -> âš ï¸ áº¢nh '{os.path.basename(image_path)}' ({file_size_mb:.2f}MB) quÃ¡ lá»›n. Báº¯t Ä‘áº§u tiá»n xá»­ lÃ½...")
                with Image.open(image_path) as img:
                    # Chuáº©n hÃ³a sang RGB (váº«n cáº§n thiáº¿t)
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # Resize má»™t cÃ¡ch tháº­n trá»ng hÆ¡n
                    # Giá»¯ láº¡i nhiá»u chi tiáº¿t hÆ¡n báº±ng cÃ¡ch dÃ¹ng max_dimension lá»›n hÆ¡n
                    if max(img.width, img.height) > max_dimension:
                        img.thumbnail((max_dimension, max_dimension))
                        print(f"      -> Resized xuá»‘ng cÃ²n {img.size[0]}x{img.size[1]}px.")

                    # NÃ©n vá»›i cháº¥t lÆ°á»£ng cao hÆ¡n
                    buffer = io.BytesIO()
                    img.save(buffer, format="JPEG", quality=quality)
                    img_bytes = buffer.getvalue()
                    
                    new_size_mb = len(img_bytes) / (1024 * 1024)
                    print(f"      -> NÃ©n xong. KÃ­ch thÆ°á»›c má»›i: {new_size_mb:.2f}MB.")
                    
                    return base64.b64encode(img_bytes).decode('utf-8')

        except FileNotFoundError:
            print(f"--- âš ï¸ Lá»—i khi xá»­ lÃ½ áº£nh: File khÃ´ng tá»“n táº¡i táº¡i '{image_path}' ---")
            return ""
        except Exception as e:
            print(f"--- âš ï¸ Lá»—i khi xá»­ lÃ½ áº£nh {image_path}: {e} ---")
            return ""

    # === HÃ€M ÄÃƒ ÄÆ¯á»¢C NÃ‚NG Cáº¤P ===
    def perform_vqa(self, image_path: str, question: str, context_text: Optional[str] = None) -> Dict[str, any]:
        """
        Thá»±c hiá»‡n VQA sá»­ dá»¥ng GPT-4o, cÃ³ thá»ƒ nháº­n thÃªm bá»‘i cáº£nh tá»« transcript.
        *** PHIÃŠN Báº¢N CÃ“ Xá»¬ LÃ Lá»–I Tá»T HÆ N VÃ€ Bá»I Cáº¢NH Má» Rá»˜NG ***
        """
        base64_image = self._preprocess_and_encode_image(image_path)
        if not base64_image:
            return {"answer": "Lá»—i: KhÃ´ng thá»ƒ xá»­ lÃ½ áº£nh", "confidence": 0.0}

        # *** Báº®T Äáº¦U NÃ‚NG Cáº¤P Táº I ÄÃ‚Y ***
        
        context_prompt_part = ""
        has_context = False
        if context_text and context_text.strip():
            has_context = True
            truncated_context = (context_text[:500] + '...') if len(context_text) > 503 else context_text
            context_prompt_part = f"""
        **Additional Context from Transcript (What was said around this moment):**
        ---
        "{truncated_context}"
        ---
        """
        
        # ThÃªm log má»›i
        if has_context:
            print(f"   -> ğŸ§  Thá»±c hiá»‡n VQA vá»›i Context: '{question}'")
        
        prompt = f"""
        Analyze the image and use the provided transcript context (if any) to answer the question in Vietnamese.
        Return a JSON object with two keys: "answer" (string) and "confidence" (float).
        {context_prompt_part}
        Question: "{question}"
        """
        
        messages = [
            {"role": "user", "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
            ]}
        ]

        try:
            response_content = self._openai_vision_call(messages, is_json=True, is_vision=True)
            
            if not response_content:
                print("--- âš ï¸ OpenAI VQA khÃ´ng tráº£ vá» ná»™i dung. ---")
                return {"answer": "KhÃ´ng thá»ƒ phÃ¢n tÃ­ch hÃ¬nh áº£nh", "confidence": 0.1}

            result = json.loads(response_content)
            return {
                "answer": result.get("answer", "KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i"),
                "confidence": float(result.get("confidence", 0.5))
            }
        except (json.JSONDecodeError, TypeError) as e:
            print(f"Lá»—i OpenAI perform_vqa (JSON parsing): {e}. Response nháº­n Ä‘Æ°á»£c: '{response_content}'")
            return {"answer": "Lá»—i Ä‘á»‹nh dáº¡ng pháº£n há»“i", "confidence": 0.0}
        except Exception as e:
            print(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh trong OpenAI perform_vqa: {e}")
            return {"answer": "Lá»—i xá»­ lÃ½ VQA", "confidence": 0.0}

    def decompose_trake_query(self, query: str) -> List[str]:
        # --- KHÃ”NG THAY Äá»”I ---
        prompt = f"""
        Decompose the Vietnamese query...
        ...
        """
        try:
            response_content = self._openai_vision_call([{"role": "user", "content": prompt}], is_json=True)
            result = json.loads(response_content)
            if isinstance(result, list):
                return result
            return [query]
        except Exception as e:
            print(f"Lá»—i OpenAI decompose_trake_query: {e}")
            return [query]