import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
import re
import torch
from tqdm import tqdm
from typing import Dict, List, Optional, Any

# Import BasicSearcher ƒë·ªÉ s·ª≠ d·ª•ng l√†m n·ªÅn t·∫£ng
from search_core.basic_searcher import BasicSearcher

# Import th∆∞ vi·ªán Gemini v√† decorator retrier
import google.generativeai as genai
from utils import gemini_api_retrier

class SemanticSearcher:
    """
    Class th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ nghƒ©a n√¢ng cao.

    Ch·ªãu tr√°ch nhi·ªám ch√≠nh cho vi·ªác t√¨m ki·∫øm d·ª±a tr√™n n·ªôi dung (KIS) v√† t√¨m ki·∫øm b·ªëi c·∫£nh cho c√°c t√°c v·ª• kh√°c.
    Bao g·ªìm c√°c b∆∞·ªõc:
    1.  Truy xu·∫•t ·ª©ng vi√™n ban ƒë·∫ßu b·∫±ng CLIP ƒëa ng√¥n ng·ªØ.
    2.  TƒÉng c∆∞·ªùng truy v·∫•n b·∫±ng Gemini ƒë·ªÉ tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† b·ªëi c·∫£nh.
    3.  T√°i x·∫øp h·∫°ng (rerank) c√°c ·ª©ng vi√™n d·ª±a tr√™n c√¥ng th·ª©c ƒëi·ªÉm k·∫øt h·ª£p 3 y·∫øu t·ªë:
        CLIP (h√¨nh ·∫£nh), Object (ƒë·ªëi t∆∞·ª£ng), v√† Semantic (ng·ªØ c·∫£nh/transcript).
    """

    def __init__(self, 
                 basic_searcher: BasicSearcher, 
                 device: str = "cuda"):
        """
        Kh·ªüi t·∫°o SemanticSearcher.

        Args:
            basic_searcher (BasicSearcher): M·ªôt instance c·ªßa BasicSearcher ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.
            device (str): Thi·∫øt b·ªã ƒë·ªÉ ch·∫°y model ('cuda' ho·∫∑c 'cpu').
        """
        print("--- üß† Kh·ªüi t·∫°o SemanticSearcher (Phi√™n b·∫£n N√¢ng cao) ---")
        self.device = device
        self.basic_searcher = basic_searcher
        # gemini_model s·∫Ω ƒë∆∞·ª£c g√°n t·ª´ b√™n ngo√†i b·ªüi MasterSearcher
        self.gemini_model: Optional[genai.GenerativeModel] = None

        print("   -> ƒêang t·∫£i m√¥ h√¨nh Bi-Encoder ti·∫øng Vi·ªát ('bkai-foundation-models/vietnamese-bi-encoder')...")
        self.semantic_model = SentenceTransformer(
            'bkai-foundation-models/vietnamese-bi-encoder', 
            device=self.device
        )
        print("--- ‚úÖ T·∫£i model Bi-Encoder th√†nh c√¥ng! ---")
        
    @gemini_api_retrier(max_retries=3, initial_delay=2)
    def _gemini_enhance_call(self, prompt: str):
        """H√†m con ƒë∆∞·ª£c "trang tr√≠", ch·ªâ ƒë·ªÉ th·ª±c hi·ªán l·ªánh g·ªçi API Gemini."""
        safety_settings = {
            "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
            "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE",
            "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
            "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE",
        }
        response = self.gemini_model.generate_content(prompt, safety_settings=safety_settings)
        return response

    def enhance_query_with_gemini(self, query: str) -> Dict[str, Any]:
        """
        S·ª≠ d·ª•ng Gemini ƒë·ªÉ ph√¢n t√≠ch m·ªôt truy v·∫•n, c√≥ th·ªÉ l√† KIS ho·∫∑c VQA.

        Returns:
            Dict[str, Any]: M·ªôt dictionary ch·ª©a 'search_context', 'specific_question', 
                            'objects_vi', 'objects_en'.
        """
        fallback_result = {
            'search_context': query,
            'specific_question': "" if "?" not in query else query,
            'objects_vi': query.split(),
            'objects_en': query.split()
        }
        
        if not self.gemini_model:
            print("--- ‚ö†Ô∏è Gemini model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. S·ª≠ d·ª•ng fallback cho enhance_query. ---")
            return fallback_result

        prompt = f"""
        You are an expert query analyzer for a Vietnamese video search system. Your task is to analyze a user query and break it down into structured components. The query can be a simple scene description (KIS) or a question about a scene (VQA).

        Return ONLY a single, valid JSON object with four keys: "search_context", "specific_question", "objects_vi", and "objects_en".

        **Rules:**
        - "search_context": A descriptive phrase in Vietnamese for finding the relevant scene. For VQA, this is the scene the question is about. For KIS, it's the query itself.
        - "specific_question": The specific question being asked. For KIS queries, this should be an empty string "".
        - "objects_vi": A list of important Vietnamese nouns/entities from the "search_context".
        - "objects_en": The direct English translation for EACH item in "objects_vi". The two lists must have the same length.

        **Example 1 (VQA):**
        Query: "Trong video quay c·∫£nh b·ªØa ti·ªác, ng∆∞·ªùi ph·ª• n·ªØ m·∫∑c v√°y ƒë·ªè ƒëang c·∫ßm ly m√†u g√¨?"
        JSON: {{"search_context": "c·∫£nh b·ªØa ti·ªác c√≥ ng∆∞·ªùi ph·ª• n·ªØ m·∫∑c v√°y ƒë·ªè", "specific_question": "c√¥ ·∫•y ƒëang c·∫ßm ly m√†u g√¨?", "objects_vi": ["b·ªØa ti·ªác", "ng∆∞·ªùi ph·ª• n·ªØ", "v√°y ƒë·ªè"], "objects_en": ["party", "woman", "red dress"]}}

        **Example 2 (KIS):**
        Query: "m·ªôt chi·∫øc xe c·ª©u h·ªèa ƒëang ch·ªØa ch√°y t√≤a nh√†"
        JSON: {{"search_context": "m·ªôt chi·∫øc xe c·ª©u h·ªèa ƒëang ch·ªØa ch√°y t√≤a nh√†", "specific_question": "", "objects_vi": ["xe c·ª©u h·ªèa", "t√≤a nh√†"], "objects_en": ["fire truck", "building"]}}

        **Query to process:** "{query}"
        **JSON:**
        """
        try:
            response = self._gemini_enhance_call(prompt)
            match = re.search(r"\{.*\}", response.text, re.DOTALL)
            if not match:
                raise ValueError("No JSON object found in Gemini response.")
            
            result = json.loads(match.group(0))
            
            if all(k in result for k in ['search_context', 'specific_question', 'objects_vi', 'objects_en']) and \
               len(result['objects_vi']) == len(result['objects_en']):
                print(f"--- ‚úÖ Ph√¢n t√≠ch truy v·∫•n th√†nh c√¥ng. Context: '{result['search_context']}' ---")
                return result

            print("--- ‚ö†Ô∏è JSON t·ª´ Gemini kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng fallback. ---")
            return fallback_result
        except Exception as e:
            print(f"--- ‚ö†Ô∏è L·ªói khi g·ªçi API Gemini ƒë·ªÉ tƒÉng c∆∞·ªùng truy v·∫•n: {e}. S·ª≠ d·ª•ng fallback. ---")
            return fallback_result

    def search(self, 
            query_text: str, 
            top_k_final: int = 12, 
            top_k_retrieval: int = 100, 
            precomputed_analysis: Optional[Dict] = None
            ) -> List[Dict[str, Any]]:
        """
        Th·ª±c hi·ªán pipeline t√¨m ki·∫øm ng·ªØ nghƒ©a ho√†n ch·ªânh cho m·ªôt context.
        *** PHI√äN B·∫¢N T·ªêI ∆ØU H√ìA: T·∫Øt progress bar v√† m√£ h√≥a object theo batch ***

        Args:
            query_text (str): B·ªëi c·∫£nh t√¨m ki·∫øm (search_context).
            top_k_final (int): S·ªë k·∫øt qu·∫£ cu·ªëi c√πng tr·∫£ v·ªÅ.
            top_k_retrieval (int): S·ªë ·ª©ng vi√™n ban ƒë·∫ßu ƒë∆∞·ª£c l·∫•y t·ª´ FAISS.
            precomputed_analysis (Optional[Dict]): K·∫øt qu·∫£ ph√¢n t√≠ch Gemini ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n tr∆∞·ªõc.

        Returns:
            List[Dict[str, Any]]: Danh s√°ch c√°c keyframe k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c x·∫øp h·∫°ng.
        """
        print(f"\n--- B·∫Øt ƒë·∫ßu Semantic Search cho context: '{query_text}' ---")
        
        # --- B∆∞·ªõc 1: Truy xu·∫•t ·ª©ng vi√™n b·∫±ng CLIP ---
        candidates = self.basic_searcher.search(query_text, top_k=top_k_retrieval)
        if not candidates:
            print("-> Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n n√†o.")
            return []

        # --- B∆∞·ªõc 2: L·∫•y th√¥ng tin Ph√¢n t√≠ch & TƒÉng c∆∞·ªùng ---
        if precomputed_analysis:
            enhanced_query = precomputed_analysis
        else:
            enhanced_query = self.enhance_query_with_gemini(query_text)
        
        rerank_keywords_en = enhanced_query.get('objects_en', [])
        rerank_context_vi = enhanced_query.get('search_context', query_text).lower().strip()
        
        # --- B∆∞·ªõc 3: T√°i x·∫øp h·∫°ng (Reranking) ---

        # --- 3a. M√£ h√≥a Context v√† Transcript (theo batch) ---
        context_vector = self.semantic_model.encode(rerank_context_vi, convert_to_tensor=True, device=self.device)
        candidate_texts = [cand.get('searchable_text', '') for cand in candidates]
        candidate_vectors = self.semantic_model.encode(
            candidate_texts, 
            convert_to_tensor=True, 
            device=self.device, 
            batch_size=128, 
            show_progress_bar=False # T·∫ÆT THANH TI·∫æN TR√åNH
        )
        semantic_scores_tensor = util.pytorch_cos_sim(context_vector, candidate_vectors)[0]

        # --- 3b. M√£ h√≥a Object (t·ªëi ∆∞u h√≥a theo batch) ---
        # Gom t·∫•t c·∫£ c√°c object t·ª´ t·∫•t c·∫£ c√°c ·ª©ng vi√™n l·∫°i
        all_detected_objects = []
        cand_object_indices = [] # L∆∞u l·∫°i index (start, end) ƒë·ªÉ map k·∫øt qu·∫£ v·ªÅ
        for cand in candidates:
            detected_objects_en_raw = cand.get('objects_detected', [])
            # Chuy·ªÉn ƒë·ªïi an to√†n sang list
            detected_objects_en = list(detected_objects_en_raw) if isinstance(detected_objects_en_raw, np.ndarray) else list(detected_objects_en_raw)
            
            start_index = len(all_detected_objects)
            all_detected_objects.extend(detected_objects_en)
            end_index = len(all_detected_objects)
            cand_object_indices.append((start_index, end_index))

        # M√£ h√≥a t·∫•t c·∫£ c√°c object trong m·ªôt l·∫ßn duy nh·∫•t
        all_object_vectors = None
        if all_detected_objects:
            all_object_vectors = self.semantic_model.encode(
                all_detected_objects, 
                convert_to_tensor=True, 
                device=self.device, 
                batch_size=256, 
                show_progress_bar=False # T·∫ÆT THANH TI·∫æN TR√åNH
            )

        # M√£ h√≥a object c·ªßa truy v·∫•n m·ªôt l·∫ßn
        query_object_vectors = None
        if rerank_keywords_en:
            query_object_vectors = self.semantic_model.encode(
                rerank_keywords_en, convert_to_tensor=True, device=self.device)

        # --- 3c. T√≠nh to√°n ƒëi·ªÉm cu·ªëi c√πng cho m·ªói ·ª©ng vi√™n ---
        reranked_results = []
        for i, cand in enumerate(candidates):
            # T√≠nh Object Score
            object_score = 0.0
            start, end = cand_object_indices[i]
            
            # L·∫•y c√°c vector ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n tr∆∞·ªõc, kh√¥ng c·∫ßn encode l·∫°i
            if query_object_vectors is not None and start < end and all_object_vectors is not None:
                detected_object_vectors = all_object_vectors[start:end]
                cosine_scores = util.pytorch_cos_sim(query_object_vectors, detected_object_vectors)
                if cosine_scores.numel() > 0:
                    max_scores_per_query_obj = torch.max(cosine_scores, dim=1).values
                    object_score = torch.mean(max_scores_per_query_obj).item()

            # T√≠nh Semantic Score
            semantic_score = (semantic_scores_tensor[i].item() + 1) / 2
            
            # T√≠nh Final Score
            w_clip, w_obj, w_semantic = 0.4, 0.3, 0.3
            normalized_clip_score = cand['clip_score']
            
            final_score = (w_clip * normalized_clip_score + 
                        w_obj * object_score + 
                        w_semantic * semantic_score)
            
            cand['final_score'] = final_score
            cand['scores'] = {'clip': normalized_clip_score, 'object': object_score, 'semantic': semantic_score}
            reranked_results.append(cand)

        # S·∫Øp x·∫øp l·∫°i d·ª±a tr√™n ƒëi·ªÉm cu·ªëi c√πng v√† tr·∫£ v·ªÅ
        reranked_results = sorted(reranked_results, key=lambda x: x['final_score'], reverse=True)
        print(f"--- ‚úÖ T√°i x·∫øp h·∫°ng cho context '{query_text}' ho√†n t·∫•t! ---")
        
        return reranked_results[:top_k_final]